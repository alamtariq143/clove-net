import sys
import boto3
import pandas as pd
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import Row
from pyspark.ml.linalg import Vectors
from functools import reduce
from pyspark.sql import DataFrame
from pyspark.sql.functions import lit
from pyspark.sql.functions import to_timestamp
from pyspark.sql.functions import udf
import pyspark.sql.functions as func 
from pyspark.sql import Window
from datetime import datetime
from functools import reduce

#collect()[0][1] is useful to select cell values

#check and deletes older parquet files
s3_client = boto3.client('s3')
BUCKET = 'cap-qa-data-lake'
PREFIX = 'pricing-app/processed/analytical_layer/al_3_1_0_enriched_transactional_alert/'

response = s3_client.list_objects_v2(Bucket=BUCKET, Prefix=PREFIX)

if 'Contents' in response.keys():
    for object in response['Contents']:
        print('Deleting', object['Key'])
        s3_client.delete_object(Bucket=BUCKET, Key=object['Key'])
        
        
# PREFIX = 'pricing-app/processed/analytical_layer/al_3_1_0_enriched_transactional_alert_data_dummy/'

# response = s3_client.list_objects_v2(Bucket=BUCKET, Prefix=PREFIX)

# if 'Contents' in response.keys():
#     for object in response['Contents']:
#         print('Deleting', object['Key'])
#         s3_client.delete_object(Bucket=BUCKET, Key=object['Key'])

PREFIX = 'pricing-app/processed/analytical_layer/al_3_1_0_enriched_transactional_alert_eq369/'

response = s3_client.list_objects_v2(Bucket=BUCKET, Prefix=PREFIX)

if 'Contents' in response.keys():
    for object in response['Contents']:
        print('Deleting', object['Key'])
        s3_client.delete_object(Bucket=BUCKET, Key=object['Key'])


sc = SparkContext()
glueContext = GlueContext(sc)
spark = SparkSession.builder.config("spark.sql.broadcastTimeout", "4000").getOrCreate()
#spark =  spark.conf.set("spark.sql.broadcastTimeout", "400")
spark = glueContext.spark_session
job = Job(glueContext)

#load in mapping file 1.4.19 
bw_map_srp = glueContext.create_dynamic_frame.from_catalog(database = "rfm_insights", table_name= "ds_2_6_19_Mapping_Portfolio_strategic_overwrite_opportunities", transformation_ctx = "datasource02")
bw_map_srp = bw_map_srp.toDF()
bw_map_srp = bw_map_srp.selectExpr('customer_soldto_number','customer_soldto_description', 'account_manager_', 'marketing_customer_sold_to','packed_material_ID','packed_material_description','PH3','PH4','produced_material_PH5','customer_product_region','ML2','suggested_price_kg_eur','suggested_cm_kg_eur','transaction_date as transaction_date_srp')
print(bw_map_srp.dtypes) 

bw_map_srp = bw_map_srp.withColumn("transaction_date_srp", current_date())
bw_map_srp = bw_map_srp.withColumn("transaction_date_srp",bw_map_srp.transaction_date_srp.cast(DateType()))
print('load in map srp')
bw_map_srp = bw_map_srp.drop_duplicates()

#load in mapping file 1.4.4
bw_map_low_cm  = glueContext.create_dynamic_frame.from_catalog(database = "rfm_insights", table_name= "ds_2_6_4_mapping_low_cm_cutoff", transformation_ctx = "datasource00")
bw_map_low_cm = bw_map_low_cm.toDF()
bw_map_low_cm = bw_map_low_cm.distinct() # drop duplicate rows
bw_map_low_cm = bw_map_low_cm.filter(~bw_map_low_cm.prod_hier_3.contains('F-'))  #only keep codes without F-

# print(0)
# print(bw_map_low_cm.count())
# bw_map_low_cm.show()

#transform matrix format to table with column cut_off_reference_region
def to_explode(df, by):

    # Filter dtypes and split into column names and type description
    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))
    # Spark SQL supports only homogeneous columns
    assert len(set(dtypes)) == 1, "All columns have to be of the same type"

    # Create and explode an array of (column_name, column_value) structs
    kvs = explode(array([
      struct(lit(c).alias("cut_off_reference_region"), col(c).alias("low_cm_cutoff_percentage")) for c in cols
    ])).alias("kvs")

    return df.select(by + [kvs]).select(by + ["kvs.cut_off_reference_region", "kvs.low_cm_cutoff_percentage"])

bw_map_low_cm_long = bw_map_low_cm.select("prod_hier_3","EMEA","NA","APAC","LATAM","CN")
bw_map_low_cm_long = to_explode(bw_map_low_cm_long, ["prod_hier_3"])

# print("test explode")
# print(bw_map_low_cm_long.count())
# bw_map_low_cm_long.show()

# join so column "prod_hier_3_desc" is included
bw_map_low_cm = bw_map_low_cm_long.join(broadcast(bw_map_low_cm.select("prod_hier_3","prod_hier_3_desc")), ["prod_hier_3"],'left')
bw_map_low_cm = bw_map_low_cm.distinct() # check if this is still necessary? 

# print("join cm")
# print(bw_map_low_cm.count())

# bw_map_low_cm.groupBy("prod_hier_3").count().show(98)
# bw_map_low_cm.show()

#call main data 2.7.1 as glue dyn frame
bw_data = glueContext.create_dynamic_frame.from_catalog(database = "rfm_insights", table_name= "ds_2_7_1_transactional_data", transformation_ctx = "datasource01")
#bw_data = glueContext.create_dynamic_frame.from_catalog(database = "rfm_insights", table_name= "ds_2_7_1_transactional_data_dummy", transformation_ctx = "datasource01")

# acc_mgr  = glueContext.create_dynamic_frame.from_catalog(database = "rfm_insights", table_name= "ds_2_6_14_mapping_accountmanager_input", transformation_ctx = "datasource8")

bw_data = bw_data.toDF()
bw_data = bw_data.select("cust_number","cust_descrp","cust_grp","cust_grp_adj","cust_segment","cust_classification","country_soldto",\
"country_grp","prod_number","prod_descrp","prod_grp","pfam","pfam_descrp","pline","pline_descrp","prod_chemistry","prod_type","market_segment",\
"market_segment_descrp", "sub_market_segment", "cxp_region","bu","ibg","ibg_descrp","sales_org","sales_org_descrp","plant","plant_descrp","account_manager","transaction_date",\
"fiscal_year","month","base_currency","transactional_vol_kg","cmstd_eur","giv_kg_eur","reb_kg_eur","nsv_kg_eur","frght_kg_eur","vpc_kg_eur","othvse_kg_eur",\
"cmstd_kg_eur","current_vpc_kg_eur","48_month_rolling","12_month_rolling","bu_scope")

bw_data = bw_data.withColumn('transaction_date',to_date(unix_timestamp('transaction_date', 'MM-dd-yyyy').cast('timestamp')))  # from string to timestamp

#create a column (overwrite if name exists)
bw_data = bw_data.withColumn("promised_volume_pricefactor",lit(None)) #create empty column, lit means column 
bw_data = bw_data.withColumn("node",lit(None)) #create empty column, lit means column

# 3.1.1 Negative margins lens
print('negative margin')
bw_data = bw_data.withColumn("suggested_price_negative_margin", when(bw_data.cmstd_kg_eur < 0, bw_data.giv_kg_eur+abs(bw_data.cmstd_kg_eur)).otherwise(None))  
# print(1)
# bw_data.select("suggested_price_negative_margin").show()
# bw_data.groupBy("suggested_price_negative_margin").count().show()

bw_data = bw_data.withColumn("suggested_cm_negative_margin", when(bw_data.cmstd_kg_eur < 0, 0.0).otherwise(None)) 
# print(2)
# bw_data.select("suggested_cm_negative_margin").show()

#3.1.2 Below floor low cutoff lens

bw_data = bw_data.withColumn("prod_hier_3", concat(col("bu"),lit("/"),col("pfam")))  # create a column prod_hier_3 in the main data, like in the mapping file  - remove it at the end


bw_map_low_cm = bw_map_low_cm.withColumnRenamed("cut_off_reference_region", "cxp_region")

bw_map_low_cm.show()

# temporarily change cxp_region to CN for china to match with low cm percentage
bw_data = bw_data.withColumn('cxp_region', when(bw_data.country_soldto == "PR of China", "CN").otherwise(bw_data.cxp_region))
bw_data = bw_data.join(broadcast(bw_map_low_cm), ["prod_hier_3","cxp_region"],"left") # add percentage to main data
print("Join 1 \n \n")
#bw_data.filter(bw_data.country_soldto == "PR of China").show(5) #collect()[0][1]
print(bw_data.count())
#bw_data.select("prod_hier_3","cxp_region","low_cm_cutoff_percentage").show(30)
#bw_data.groupBy("low_cm_cutoff_percentage").count().show()
bw_data = bw_data.withColumn('cxp_region', when(bw_data.country_soldto == "PR of China", "APAC").otherwise(bw_data.cxp_region)) # change china CN back to APAC


bw_data = bw_data.withColumnRenamed('12_month_rolling', 'twelve_month_rolling')

#calculate average percentage in a column with nested if else statement - don't forget to remove it at the end
bw_data = bw_data.withColumn("average_percentage", when(bw_data.twelve_month_rolling == "No", None).otherwise(when(bw_data.giv_kg_eur==0,0).otherwise(bw_data.cmstd_kg_eur/bw_data.giv_kg_eur)))

# print("Calculations percentage ")
# bw_data.select("prod_hier_3","cfxp_region","low_cm_cutoff_percentage","transaction_date","twelve_month_rolling","giv_kg_eur","cmstd_kg_eur","average_percentage").show(50)

# calculate suggested price below floor
bw_data = bw_data.withColumn("suggested_cm_low_cutoff", when(bw_data.twelve_month_rolling == "No", None).otherwise(when(bw_data.average_percentage <= bw_data.low_cm_cutoff_percentage, bw_data.low_cm_cutoff_percentage*bw_data.giv_kg_eur).otherwise(None))) 
bw_data = bw_data.withColumn("suggested_price_low_cutoff", when(bw_data.twelve_month_rolling == "No", None).otherwise(when(bw_data.average_percentage <= bw_data.low_cm_cutoff_percentage, bw_data.suggested_cm_low_cutoff - bw_data.cmstd_kg_eur + bw_data.giv_kg_eur).otherwise(None)))

bw_data = bw_data.drop("prod_hier_3_desc")
bw_data = bw_data.drop("prod_hier_3")
#bw_data = bw_data.drop("low_cm_cutoff_percentage")
#bw_data = bw_data.drop("average_percentage") 



# End of pipe account swap
# acc_mgr = acc_mgr.toDF().selectExpr("ml2 as ibg","customer as cust_number", "sales_org", "am_responsible")
# acc_mgr_cust_specic = acc_mgr.drop_duplicates(subset=['cust_number'])
# acc_mgr_all_cust = acc_mgr.filter(acc_mgr.cust_number.isNull()).drop('cust_number')

# bw_data = bw_data.join(acc_mgr_cust_specic, how='left', on=['ibg', 'cust_number', 'sales_org'])
# bw_data = bw_data.withColumn("account_manager", when(bw_data.am_responsible.isNull(), bw_data.account_manager).otherwise(bw_data.am_responsible))
# bw_data = bw_data.drop("am_responsible")

# bw_data = bw_data.join(acc_mgr_all_cust, how='left', on=['ibg', 'sales_org'])
# bw_data = bw_data.withColumn("account_manager", when(bw_data.am_responsible.isNull(), bw_data.account_manager).otherwise(bw_data.am_responsible))
# bw_data = bw_data.drop("am_responsible")


print("table after 3.1.2 /n /n")
print(bw_data.count())

# #3.1.3 RM passthrough lens
#step 1
bw_data = bw_data.withColumn("record_id", monotonically_increasing_id())
#bw_data.show(10)

bw_12M = bw_data.filter(bw_data.twelve_month_rolling == "Yes")

bw_12M = bw_12M.select("record_id","account_manager","transaction_date","cust_number","cust_descrp","cxp_region","bu","ibg","prod_number","prod_descrp","transactional_vol_kg","giv_kg_eur","vpc_kg_eur","cmstd_kg_eur","sales_org")
print("filter 12 M \n \n")
bw_12M.show(5)
a = bw_12M.groupBy("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org").agg(sum("transactional_vol_kg"))
b = bw_12M.groupBy("cust_number","cxp_region","bu","ibg","prod_number","sales_org").agg(sum("transactional_vol_kg"))
print(a.count()) #12582 -> this is how much combinations there are in 12M
print(b.count()) #  -> this is how much combinations there are in 12M

#step 2 add column max_transaction_date = current month
max_value=lit(bw_12M.agg(max("transaction_date")).collect()[0][0])
bw_12M = bw_12M.withColumn("max_transaction_date",max_value)

# step 3 match max transactiondate to find transactions in max month and calculate
bw_equal = bw_12M.filter(bw_12M.transaction_date == bw_12M.max_transaction_date)  # is empty, because 10/1/2021 is latest transaction date
bw_equal = bw_equal.withColumn("giv_kg_eur_past", bw_equal.giv_kg_eur)
bw_equal = bw_equal.withColumn("vpc_kg_eur_ave", bw_equal.vpc_kg_eur)
bw_equal = bw_equal.withColumn("priority", lit(1))
print("equal df \n \n")
print(bw_equal.count())
#bw_equal.show(40)

# print("show duplicates \n \n")
# df1=bw_equal.groupBy("cust_number","cxp_region","bu","prod_number","max_transaction_date","account_manager","sales_org").count().filter("count > 1")
# df1.drop('count').show()

# step 4 # calculate vpc_kg_eur_ave and giv_kg_eur_past for 3,6,9 months
print('bw_3M \n \n')
bw_3M = bw_12M.filter((month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -3))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -2))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -1))))
print(bw_3M.count()) # 11481
bw_3M.groupBy("transaction_date").count().show()
bw_3M_vpc = bw_3M.groupBy("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org").agg(avg("vpc_kg_eur")).withColumnRenamed('avg(vpc_kg_eur)','vpc_kg_eur_ave')
print(bw_3M_vpc.count()) #6784
bw_3M_giv = bw_3M.groupBy("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org").agg(max('transaction_date')).withColumnRenamed('max(transaction_date)','transaction_date')
print(bw_3M_giv.count()) #6784
bw_3M_giv = bw_3M_giv.join(broadcast(bw_3M), ["cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org","transaction_date"], 'left')
print(bw_3M_giv.count()) #6969 -> small increase, assumption: because of double rows transaction date bc of plants

bw_3M_giv = bw_3M_giv.withColumn("giv_kg_eur_past", bw_3M_giv.giv_kg_eur).select("cust_number","cxp_region","bu","ibg","prod_number","sales_org","transaction_date","account_manager","giv_kg_eur_past")
print(bw_3M_giv.count()) #6969
# #create a df for the combi's 
# bw_3Mf = bw_3M_vpc.join(broadcast(bw_3M_giv), ["cust_number","cxp_region","bu","prod_number","sales_org","account_manager"], 'left')
# print(bw_3Mf.count()) # 4149 
# bw_3Mf.show(5)
# bw_3Mf.groupBy("vpc_kg_eur_ave").count().show()
# df1=bw_3Mf.groupBy("cust_number","cxp_region","bu","prod_number","max_transaction_date","account_manager","sales_org").count().filter("count > 1")
# df1.drop('count').show()

#creates a df as big as 3M
print("join 3M and 3M vpc")
print(bw_3M_vpc.columns)
bw_3M = bw_3M.join(broadcast(bw_3M_vpc), ["cust_number","cxp_region","bu","ibg","prod_number","sales_org","account_manager"], 'left')
print(bw_3M.count()) #11481
bw_3M = bw_3M.join(broadcast(bw_3M_giv), ["cust_number","cxp_region","bu","ibg","prod_number","transaction_date","sales_org","account_manager"], 'left')   # hier twijfel ik over transaction date
bw_3M = bw_3M.withColumn("priority", lit(3))
print(bw_3M.count()) #11915 -> explodes because of lines with multiple lines per combi per transaction date (I checked in excel)
print("show duplicates 3M \n \n")

print('bw_6M \n \n')
bw_6M = bw_12M.filter((month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -6))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -5))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -4))) |\
(month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -3))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -2))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -1))))
print(bw_6M.count()) #23597
bw_6M_vpc = bw_6M.groupBy("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org").agg(avg("vpc_kg_eur")).withColumnRenamed('avg(vpc_kg_eur)','vpc_kg_eur_ave')
print(bw_6M_vpc.count()) #9543 -> very big loss, due to aggregation
bw_6M_giv = bw_6M.groupBy("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org").agg(max('transaction_date')).withColumnRenamed('max(transaction_date)','transaction_date')
print(bw_6M_giv.count()) #9543
bw_6M_giv = bw_6M_giv.join(broadcast(bw_6M), ["cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org","transaction_date"], 'left')
print(bw_6M_giv.count()) #9752 -> small gain because of duplicates? 

bw_6M_giv = bw_6M_giv.withColumn("giv_kg_eur_past", bw_6M_giv.giv_kg_eur).select("cust_number","cxp_region","bu","ibg","prod_number","sales_org","transaction_date","account_manager","giv_kg_eur_past")
print(bw_6M_giv.count()) #9752
# #create a df for the combi's 
# bw_6Mf = bw_6M_vpc.join(broadcast(bw_6M_giv), ["cust_number","cxp_region","bu","prod_number","sales_org","account_manager"], 'left')
# print(bw_6Mf.count()) # 8407?  
# bw_6Mf.show(5)
# bw_6Mf.groupBy("vpc_kg_eur_ave").count().show()
# df1=bw_6Mf.groupBy("cust_number","cxp_region","bu","prod_number","max_transaction_date","account_manager").count().filter("count > 1")
# df1.drop('count').show()

# creates a df as big as 6M
bw_6M = bw_6M.join(broadcast(bw_6M_vpc), ["cust_number","cxp_region","bu","ibg","prod_number","sales_org","account_manager"], 'left')
print(bw_6M.count()) #23597
bw_6M = bw_6M.join(broadcast(bw_6M_giv), ["cust_number","cxp_region","bu","ibg","prod_number","transaction_date","sales_org","account_manager"], 'left')  # if you remove trans_date, even 19144 rows instead of 17394
bw_6M = bw_6M.withColumn("priority", lit(6))
print(bw_6M.count()) #24081 -> explodes because of lines with multiple lines per combi per transaction date (I checked in excel)
print("show duplicates 6M \n \n")

print('bw_9M \n \n')
bw_9M = bw_12M.filter((month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -9))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -8))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -7))) |\
(month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -6))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -5))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -4))) |\
(month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -3))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -2))) | (month(col("transaction_date")) == month(add_months(bw_12M.max_transaction_date, -1))))
print(bw_9M.count()) #36160
bw_9M_vpc = bw_9M.groupBy("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org").agg(avg("vpc_kg_eur")).withColumnRenamed('avg(vpc_kg_eur)','vpc_kg_eur_ave')
print(bw_9M_vpc.count()) #11368
# find latest date and select giv_kg_eur
bw_9M_giv = bw_9M.groupBy("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org").agg(max('transaction_date')).withColumnRenamed('max(transaction_date)','transaction_date')
print(bw_9M_giv.count()) #11368
bw_9M_giv = bw_9M_giv.join(broadcast(bw_9M), ["cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org","transaction_date"], 'left')
print(bw_9M_giv.count()) #11593

bw_9M_giv = bw_9M_giv.withColumn("giv_kg_eur_past", bw_9M_giv.giv_kg_eur).select("cust_number","cxp_region","bu","ibg","prod_number","sales_org","transaction_date","account_manager","giv_kg_eur_past")
print(bw_9M_giv.count()) #11593
# #create a df for the combi's 
# bw_9Mf = bw_9M_vpc.join(broadcast(bw_9M_giv), ["cust_number","cxp_region","bu","prod_number","sales_org","account_manager"], 'left')
# print(bw_9Mf.count()) # 29079?  
# bw_9Mf.show(5)
# bw_9Mf.groupBy("vpc_kg_eur_ave").count().show()
# df1=bw_9Mf.groupBy("cust_number","cxp_region","bu","prod_number","max_transaction_date","account_manager","sales_org").count().filter("count > 1")
# df1.drop('count').show()

# creates a df as big as 9M
bw_9M = bw_9M.join(broadcast(bw_9M_vpc), ["cust_number","cxp_region","bu","ibg","prod_number","sales_org","account_manager"], 'left')
print(bw_9M.count()) #36160
bw_9M = bw_9M.join(broadcast(bw_9M_giv), ["cust_number","cxp_region","bu","ibg","prod_number","transaction_date","sales_org","account_manager"], 'left')  # giv_kg_eur_past is null
bw_9M = bw_9M.withColumn("priority", lit(9))
print(bw_9M.count()) #36680 -> explodes a bit
# bw_9M.show(5)
# bw_9M.groupBy("vpc_kg_eur_ave").count().show()
# bw_9M.groupBy("giv_kg_eur_past").count().show()

# bw_9M.filter((bw_9M.cust_number == "0008545431") & (bw_9M.prod_number ==  87462529)).show()

# step 5 & 6 join records from step 3 & 4 and sort
print("join equal, 3,6,9 \n\n")
# bw_equal.printSchema()
# bw_3M.printSchema()
# bw_6M.printSchema()
# bw_9M.printSchema()

# create one dataframe with all rows from 3M, 6M and 9M -> maybe nicer
bw_eq369 = bw_equal.unionByName(bw_3M)
print(bw_eq369.count()) # 11915
bw_eq369 = bw_eq369.unionByName(bw_6M)
print(bw_eq369.count()) # 35996
bw_eq369 = bw_eq369.unionByName(bw_9M)
print(bw_eq369.count()) # 72676   # more than original 12M dataframe, because transactions can be in 3M as well as 6M as well as 9M 
bw_eq369 = bw_eq369.distinct()

# prioritize rows with lower priority
print('final bw_eq369 \n\n') 
bw_eq369_final = bw_eq369.groupBy("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org").agg(min("priority")).withColumnRenamed('min(priority)','priority')
bw_eq369_final.show(3)
print("show duplicates \n \n") # there are no duplicates, which is nice

# is this needed? yes to get the lost columns back 
print("bw_eq369_final")
# bw_eq369_final.filter((bw_eq369_final.cust_number == "0003450343") & (bw_eq369_final.prod_number == 87836070) & (bw_eq369_final.account_manager == "MINCL")).show()

#this join leads to too many rows. 
bw_eq369 = bw_eq369.select("cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org","priority","vpc_kg_eur","vpc_kg_eur_ave","giv_kg_eur","giv_kg_eur_past","cmstd_kg_eur","transaction_date")

bw_eq369_final = bw_eq369_final.join(broadcast(bw_eq369), ["cust_number","cxp_region","bu","ibg","prod_number","account_manager","sales_org","priority"], 'left')
print(bw_eq369_final.count())

# only keep transaction with latest date by filtering on giv_kg_eur_past
print('trial')
bw_eq369_final = bw_eq369_final.filter(bw_eq369_final.giv_kg_eur_past.isNotNull())
print(bw_eq369_final.count())
bw_eq369_final = bw_eq369_final.filter(~isnan(bw_eq369_final.giv_kg_eur_past))
print(bw_eq369_final.count())

bw_eq369_final.show(5)
print(bw_eq369_final.count())

print("sorted, 3,6,9")
bw_eq369_final = bw_eq369_final.sort(bw_eq369_final.vpc_kg_eur_ave.desc()).coalesce(1)
print(bw_eq369_final.count()) # small increase in number of rows because of plants
bw_eq369_final = bw_eq369_final.distinct()

print(bw_eq369_final.count())


# #step 7 calculate suggested_price_rm_passthrough and suggested_cm_rm_passthrough
bw_eq369 = bw_eq369_final
print(bw_eq369.columns)
bw_eq369 = bw_eq369.withColumn("delta_vpc_kg_eur", bw_eq369.vpc_kg_eur - bw_eq369.vpc_kg_eur_ave)
bw_eq369 = bw_eq369.withColumn("delta_giv_kg_eur", bw_eq369.giv_kg_eur - bw_eq369.giv_kg_eur_past)
bw_eq369 = bw_eq369.withColumn("delta", when(bw_eq369.delta_giv_kg_eur < bw_eq369.delta_vpc_kg_eur, bw_eq369.delta_vpc_kg_eur-bw_eq369.delta_giv_kg_eur).otherwise(None))
bw_eq369 = bw_eq369.withColumn("delta", when(bw_eq369.delta <= 0, None).otherwise(bw_eq369.delta))

bw_eq369 = bw_eq369.withColumn("suggested_cm_rm_passthrough", when((bw_eq369.cmstd_kg_eur+bw_eq369.delta) > 0, bw_eq369.cmstd_kg_eur + bw_eq369.delta).otherwise(None))
bw_eq369 = bw_eq369.withColumn("suggested_price_rm_passthrough", when((isnan(bw_eq369.suggested_cm_rm_passthrough) | bw_eq369.suggested_cm_rm_passthrough.isNull()),None).otherwise(bw_eq369.giv_kg_eur + bw_eq369.delta))
bw_eq369 = bw_eq369.withColumn("suggested_price_rm_passthrough", when(round(bw_eq369.suggested_price_rm_passthrough,2) <= round(bw_eq369.giv_kg_eur,2),None).otherwise(bw_eq369.suggested_price_rm_passthrough))
bw_eq369 = bw_eq369.withColumn("suggested_cm_rm_passthrough", when((isnan(bw_eq369.suggested_price_rm_passthrough) | bw_eq369.suggested_price_rm_passthrough.isNull()),None).otherwise(bw_eq369.suggested_cm_rm_passthrough))

# bw_eq369_df = DynamicFrame.fromDF(bw_eq369, glueContext, "bw_eq369_df")
# datasink6 = glueContext.write_dynamic_frame.from_options(frame = bw_eq369_df, connection_type = "s3", connection_options = {"path": "s3://cap-qa-data-lake/pricing-app/processed/analytical_layer/al_3_1_0_enriched_transactional_alert_eq369"}, format = "parquet", transformation_ctx = "datasink6")

print("step 7 calculations \n\n")
print(bw_eq369.count())
bw_eq369.show(5)
#bw_eq369 = bw_eq369.drop('record_id','priority','delta_vpc_kg_eur',"delta_giv_kg_eur","delta","transaction_date","cust_descrp","prod_descrp","transactional_vol_kg","giv_kg_eur","vpc_kg_eur","cmstd_kg_eur","max_transaction_date","giv_kg_eur_past","vpc_kg_eur_ave")
bw_eq369 = bw_eq369.drop('record_id','priority',"transaction_date","cust_descrp","prod_descrp","transactional_vol_kg","giv_kg_eur","vpc_kg_eur","cmstd_kg_eur","max_transaction_date")

# #step 8
#join back to the main table bw_data - explodes like crazy
print("join back to bw_data") 
bw_data.show(5)
print(bw_data.count())
bw_data = bw_data.join(broadcast(bw_eq369), ["cust_number","cxp_region","bu","ibg","prod_number","sales_org","account_manager"], 'left') # make sure only the two new columns are addded, or only use record Id for match
# bw_data = bw_data.join(broadcast(bw_eq369), ["cust_number","cxp_region","bu","prod_number","account_manager",'record_id','priority','delta_vpc_kg_eur',"delta_giv_kg_eur","delta","transaction_date",\
# "cust_descrp","prod_descrp","transactional_vol_kg","giv_kg_eur","vpc_kg_eur","cmstd_kg_eur","sales_org","max_transaction_date","giv_kg_eur_past","vpc_kg_eur_ave"], 'left')



bw_data = bw_data.drop('record_id')
#bw_data = bw_data.drop('record_id','vpc_kg_eur_ave')

# Correction for all the suggested price alerst
bw_data = bw_data.withColumn("suggested_price_negative_margin", when(round(bw_data.suggested_price_negative_margin,2) <= bw_data.giv_kg_eur, None).otherwise(bw_data.suggested_price_negative_margin))
bw_data = bw_data.withColumn("suggested_price_low_cutoff", when(round(bw_data.suggested_price_low_cutoff,2) <= bw_data.giv_kg_eur, None).otherwise(bw_data.suggested_price_low_cutoff))
bw_data = bw_data.withColumn("suggested_price_rm_passthrough", when(round(bw_data.suggested_price_rm_passthrough,2) <= bw_data.giv_kg_eur, None).otherwise(bw_data.suggested_price_rm_passthrough))

# 
# 3.1.4 Outlier lens
print("--" * 40)
print('\n\n \n OUTLIER LENS \n \n \n')
bw_outl = bw_data.filter((bw_data.transactional_vol_kg > 0) & (bw_data.cmstd_eur > 0))

#bw_outl = bw_data.filter((bw_data.transactional_vol_kg > 0) & (bw_data.cmstd_eur > 0) & (bw_data.cxp_region != 'DFM') & (bw_data.market_segment_descrp != 'ZP') & (bw_data.market_segment_descrp != 'ZJ') & (bw_data.market_segment_descrp != 'H') & (bw_data.market_segment_descrp != 'ZG')  & (bw_data.market_segment_descrp != 'ZI') & (bw_data.market_segment_descrp != 'TP'))

# why is this needed? 
#bw_outl = bw_outl.withColumn('cxp_region', when(bw_outl.country_soldto == "Israel", "EMA").otherwise(bw_outl.cxp_region))
bw_outl = bw_outl.filter(bw_outl.twelve_month_rolling == "Yes")

bw_outl = bw_outl.sort(bw_outl.fiscal_year.desc(), bw_outl.month.desc()).coalesce(1)
print(bw_outl.count())

bw_outl = bw_outl.drop_duplicates(["cust_number", "cust_grp_adj", "prod_number", "prod_grp", "cust_segment", "market_segment", "cxp_region"])
print(bw_outl.count())

bw_cluster = bw_outl.groupBy("prod_grp", "cust_segment", "market_segment", "cxp_region").agg(sum("transactional_vol_kg").alias("sum_transactional_vol_kg"))
bw_cluster = bw_cluster.withColumn('cluster_id', monotonically_increasing_id())
bw_cluster = bw_cluster.select("prod_grp", "cust_segment", "market_segment", "cxp_region", "cluster_id")

bw_outl = bw_outl.join(broadcast(bw_cluster), ["prod_grp", "cust_segment", "market_segment", "cxp_region"], 'left')

## rethink
bw_out = bw_outl.groupBy("cluster_id", "cust_segment", "cust_grp_adj", "market_segment", "prod_grp", "cxp_region").agg(sum("transactional_vol_kg").alias("sum_transactional_vol_kg"), sum("cmstd_eur").alias("sum_cmstd_eur"))

bw_out = bw_out.withColumn("cxps", concat(bw_out.cust_grp_adj, bw_out.market_segment, bw_out.prod_grp, bw_out.cxp_region))

cxps_no = bw_out.groupBy("cluster_id").agg(countDistinct("cxps").alias("no_of_unique_cxps"))

bw_out = bw_out.join(broadcast(cxps_no), "cluster_id", 'left')

bw_out = bw_out.filter(bw_out.no_of_unique_cxps >= 4)


bw_out = bw_out.withColumn('avg_cm', when(bw_out.sum_transactional_vol_kg == 0, 0.0).otherwise(bw_out.sum_cmstd_eur / bw_out.sum_transactional_vol_kg))

bw_out = bw_out.select("cluster_id", "cust_segment", "cust_grp_adj", "market_segment", "prod_grp", "cxp_region", "avg_cm")

w = Window.partitionBy(bw_out.cluster_id).orderBy(bw_out.avg_cm) #assuming default ascending order

bw_out = bw_out.withColumn('percentile', func.percent_rank().over(w))

thresholds = bw_out.groupBy("cluster_id").agg(func.expr('percentile(avg_cm, .5)').alias('small_thresh'), func.expr('percentile(avg_cm, .25)').alias('medium_thresh'), func.expr('percentile(avg_cm, .1)').alias('large_thresh'))

bw_out = bw_out.join(broadcast(thresholds), "cluster_id", 'left')

bw_out = bw_out.withColumn("suggested_cm_outlier",when(((bw_out.cust_segment == "Large") & (bw_out.percentile < 0.1)), bw_out.large_thresh).when(((bw_out.cust_segment == "Medium") & (bw_out.percentile < 0.25)), bw_out.medium_thresh).when(((bw_out.cust_segment == "Small") & (bw_out.percentile < 0.5)), bw_out.small_thresh).otherwise(None))

bw_out_val = bw_outl.join(broadcast(bw_out), ["cust_segment", "cust_grp_adj", "market_segment", "prod_grp", "cxp_region"], 'left')

bw_out = bw_out.select("cust_segment", "cust_grp_adj", "market_segment", "prod_grp", "cxp_region", "suggested_cm_outlier")

bw_outl = bw_outl.join(broadcast(bw_out), ["cust_segment", "cust_grp_adj", "market_segment", "prod_grp", "cxp_region"], 'left')


#bw_outl = bw_outl.withColumn('cxp_region', when(bw_outl.country_soldto == "Israel", "EMEA").otherwise(bw_outl.cxp_region)) # change back to EMEA

bw_outl = bw_outl.withColumn("suggested_price_outlier", when(((bw_outl.suggested_cm_outlier.isNotNull()) & (~isnan(bw_outl.suggested_cm_outlier))), (bw_outl.giv_kg_eur - bw_outl.cmstd_kg_eur) + bw_outl.suggested_cm_outlier).otherwise(None))

bw_outl = bw_outl.select("cust_number","bu","ibg","prod_number","sales_org","account_manager","market_segment", "prod_grp", "cxp_region", "cust_segment",'transaction_date','suggested_price_outlier','suggested_cm_outlier')

bw_outl = bw_outl.withColumnRenamed('transaction_date','transaction_date')
bw_data = bw_data.withColumnRenamed('transaction_date','transaction_date')


print("\n \n")

print(bw_data.count())

bw_data = bw_data.join(broadcast(bw_outl),["cust_number","bu","ibg","prod_number","sales_org","account_manager","market_segment", "prod_grp", "cxp_region", "cust_segment","transaction_date"], 'left')

bw_data = bw_data.withColumnRenamed('twelve_month_rolling','12_month_rolling')

print("\n \n")

print(bw_data.count())

#export just to check what bw_outl looks like before joining it back to bw_data
bw_eq369_df = DynamicFrame.fromDF(bw_out_val, glueContext, "bw_eq369_df")
datasink6 = glueContext.write_dynamic_frame.from_options(frame = bw_eq369_df, connection_type = "s3", connection_options = {"path": "s3://cap-qa-data-lake/pricing-app/processed/analytical_layer/al_3_1_0_enriched_transactional_alert_eq369"}, format = "parquet", transformation_ctx = "datasink6")


#bw_outl = bw_outl.select("cust_number","cxp_region","bu","ibg","prod_number","sales_org","account_manager",'transaction_date','suggested_price_outlier','suggested_cm_outlier')


# bw_eq369_df = DynamicFrame.fromDF(bw_data, glueContext, "bw_eq369_df")
# datasink6 = glueContext.write_dynamic_frame.from_options(frame = bw_eq369_df, connection_type = "s3", connection_options = {"path": "s3://cap-qa-data-lake/pricing-app/processed/analytical_layer/al_3_1_0_enriched_transactional_alert_eq369"}, format = "parquet", transformation_ctx = "datasink6")


# drop the columns I made 
#bw_data = bw_data.drop('low_price_outlier','large_customer_thresholds','medium_customer_thresholds','small_customer_thresholds','n_dff_cxp',"sum_transactional_vol_kg","sum_cmstd_eur",'avg_cm','id_cxp')


# 3.1.7 Strategic repositioning
# join mapping file 1.4.19 and bw_data
print('SRP test')
print(bw_data.count())
bw_data = bw_data.join(broadcast(bw_map_srp), [bw_map_srp.customer_soldto_number == bw_data.cust_number, bw_map_srp.customer_soldto_description	== bw_data.cust_descrp, bw_map_srp.account_manager_ == bw_data.account_manager, \
bw_map_srp.marketing_customer_sold_to == bw_data.cust_grp_adj, bw_map_srp.packed_material_ID == bw_data.prod_number, bw_map_srp.packed_material_description == bw_data.prod_descrp, bw_map_srp.PH3 == bw_data.pfam, bw_map_srp.PH4 == bw_data.pline, \
bw_map_srp.produced_material_PH5 == bw_data.prod_grp, bw_map_srp.customer_product_region == bw_data.cxp_region, bw_map_srp.ML2 == bw_data.ibg, bw_map_srp.transaction_date_srp == bw_data.transaction_date], 'left') 
print(bw_data.count())
#drop the double columns afterwards
bw_data = bw_data.drop('transaction_date_srp','customer_soldto_number','customer_soldto_description', 'account_manager_', 'marketing_customer_sold_to','packed_material_ID','packed_material_description','PH3','PH4','produced_material_PH5','customer_product_region','ML2')
print(bw_data.count())

bw_data = bw_data.withColumn("suggested_price_SRP", bw_data.suggested_price_kg_eur)
bw_data = bw_data.withColumn("suggested_cm_SRP", bw_data.suggested_cm_kg_eur)
bw_data = bw_data.drop('suggested_price_kg_eur', 'suggested_cm_kg_eur')


#bw_data = bw_data.withColumnRenamed("suggested_price_SRP","suggested_price_strategic_repositioning").withColumnRenamed("suggested_cm_SRP","suggested_cm_strategic_repositioning")

bw_data = bw_data.withColumn("upload_date", current_timestamp())
bw_data = bw_data.withColumn("source_system", lit("enriched_transactional_alert")) #same as title

print('final table bw_data \n \n ')
bw_data = bw_data.drop_duplicates()
# print("check negative margin")
# bw_data.filter((bw_data.average_percentage.isNotNull()) & (bw_data.suggested_cm_negative_margin.isNotNull()) & (bw_data.suggested_price_negative_margin.isNotNull())).show()
# print("check low cutoff")
# bw_data.filter((bw_data.average_percentage.isNotNull()) & (bw_data.suggested_cm_low_cutoff.isNotNull()) & (bw_data.suggested_price_low_cutoff.isNotNull())).show()  


#final pyspark df to gluedynamic frame
bw_data_df = DynamicFrame.fromDF(bw_data, glueContext, "bw_data_df")
#safe as parquet file - rename bucket
datasink4 = glueContext.write_dynamic_frame.from_options(frame = bw_data_df, connection_type = "s3", connection_options = {"path": "s3://cap-qa-data-lake/pricing-app/processed/analytical_layer/al_3_1_0_enriched_transactional_alert"}, format = "parquet", transformation_ctx = "datasink4")

job.commit()